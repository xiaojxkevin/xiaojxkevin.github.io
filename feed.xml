<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://xiaojxkevin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xiaojxkevin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-04T08:21:12+00:00</updated><id>https://xiaojxkevin.github.io/feed.xml</id><title type="html">Jinxi’s Home</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Matrix Basis</title><link href="https://xiaojxkevin.github.io/blog/2024/matrix-basis/" rel="alternate" type="text/html" title="Matrix Basis"/><published>2024-05-04T00:00:00+00:00</published><updated>2024-05-04T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/matrix-basis</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/matrix-basis/"><![CDATA[<style>r{color:Red}b{color:Blue}g{color:Green}</style> <p><strong><em>This blog is only used as a reminder.</em></strong></p> <p><strong><em>Look for <a href="https://si231.sist.shanghaitech.edu.cn/">https://si231.sist.shanghaitech.edu.cn/</a> and <a href="https://github.com/xiaojxkevin/Linear-Algebra-1-fall23">https://github.com/xiaojxkevin/Linear-Algebra-1-fall23</a> for more details</em></strong></p> <h2 id="notations-and-conventions">Notations and Conventions</h2> <h3 id="trace">Trace</h3> <ol> <li> \[tr(A^T) = tr(A)\] </li> <li> \[tr(A+B) = tr(A) + tr(B)\] </li> <li> \[tr(AB) = tr(BA)\] <ul> <li> \[tr(xy^T) = x^Ty\] </li> <li> \[tr(ABC) = tr(BCA) = tr(CAB)\] </li> </ul> </li> </ol> <h3 id="band-matrices">Band matrices</h3> <p>\(:=\) A matrix \(A \in \mathbb{R}^{n\times n}\) is said to be a band matrix if all matrix elements are zero outside a diagonal ordered band, i.e.</p> \[a_{ij} = 0 \quad \text{if} \quad i &gt; j + p \ \text{or} \ j &gt; i + q\] <p>where \(p, q \geq 0\)</p> <h3 id="toeplitz-matrices">Toeplitz matrices</h3> <p>\(:=\) matrices with constant diagonals (may not be square)</p> \[A=\begin{bmatrix} a_{0} &amp; a_{-1} &amp; a_{-2} &amp; \cdots &amp; \cdots &amp; a_{-( n-1)}\\ a_{1} &amp; a_{0} &amp; \ddots &amp; \ddots &amp; &amp; \vdots \\ a_{2} &amp; a_{1} &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; a_{1} &amp; \ddots &amp; a_{-1} &amp; a_{-2}\\ \vdots &amp; &amp; \ddots &amp; a_{1} &amp; a_{0} &amp; a_{-1}\\ a_{n-1} &amp; \cdots &amp; \cdots &amp; a_{2} &amp; a_{1} &amp; a_{0} \end{bmatrix}\] <h3 id="involutory-matrices">Involutory matrices</h3> <p>\(:=\) matrix \(A\) is involutory if and only if \(A^2=I\)</p> <h3 id="idempotent--matrices">Idempotent matrices</h3> <p>\(:=\) matrix \(A\) is idempotent if and only if \(A^2=A\)</p> <h2 id="rank">Rank</h2> <ol> <li> \[rank(A) = rank(A^T) = rank(A^TA)\] </li> <li>\(rank(AB) \leq \min\{rank(A), rank(B)\}\). And the equality holds when \(A\) has full row rank or \(B\) has full column rank.</li> <li>For \(A \in \mathbb{R}^{m\times p}\) and \(B \in \mathbb{R}^{p\times n}\), \(rank(AB) \geq rank(A) + rank(B) - n\)</li> <li>\(A\) is said to have <b>low rank</b> when its rank is significantly less than the maximum rank possible for the matrix.</li> </ol> <h2 id="orthogonal">Orthogonal</h2> <p>A matrix is said to be orthogonal(<b>unitary</b>) if it is real(complex), square and columns are orthonormal.</p> <h3 id="permutation-matrix">Permutation matrix</h3> <p>\(:=\) \(Q\) has exactly one element equal to 1 in each row and each column.</p> <p>As a result, \(Q^TQ=I\) since</p> \[[Q^TQ]_{ij} = \sum_{k=1}^n [Q^T]_{ik}[Q]_{kj} = \sum_{k=1}^n [Q^T]_{ki}[Q]_{kj} = \begin{cases} 1, &amp; i = j\\ 0, &amp; \text{otherwise} \end{cases}\] <h2 id="multiplication">Multiplication</h2> <p>Define \(A \in \mathbb{R}^{m\times p}\) and \(B \in \mathbb{R}^{p\times n}\), \(AB\) is equivalent to</p> <ol> <li>Performing column combinations based on columns of \(A\) with elements in the column of \(B\) as coefficients.</li> <li>Performing row combinations based on rows of \(B\) with elements in the row of \(A\) as coefficients.</li> </ol> \[\begin{array}{l} AB=A\begin{bmatrix} B_{1} &amp; \dotsc &amp; B_{n} \end{bmatrix} =\begin{bmatrix} \sum _{j=1}^{p} B_{j1} A_{j} &amp; \cdots &amp; \sum _{j=1}^{p} B_{jn} A_{j} \end{bmatrix}\\ AB=\begin{bmatrix} a_{1}^{T} B\\ \vdots \\ a_{m}^{T} B \end{bmatrix} =\begin{bmatrix} \sum _{i=1}^{p} a_{1i} b_{i}^{T}\\ \vdots \\ \sum _{i=1}^{p} a_{mi} b_{i}^{T} \end{bmatrix}\\ AB\ =\ \begin{bmatrix} a_{1}^{T}\\ \vdots \\ a_{m}^{T} \end{bmatrix}\begin{bmatrix} B_{1} &amp; \dotsc &amp; B_{n} \end{bmatrix}\\ AB\ =\ \sum _{i=1}^{p} Ae_{i} e_{i}^{T} B=\sum _{i=1}^{p} A_{i} b_{i}^{T} \end{array}\] <h3 id="schur-complement">Schur complement</h3> <p>Let</p> \[M\ =\ \begin{bmatrix} A &amp; B\\ C &amp; D \end{bmatrix}\] <p>where \(A\in \mathbb{R}^{m\times m}, B\in \mathbb{R}^{m\times n}, C\in\mathbb{R}^{n\times m}\) and \(D\in \mathbb{R}^{n\times n}\).</p> <ol> <li> <p>If \(A\) is invertible, then the Schur complement of \(A\) in \(M\) is defined by</p> \[S_A = D - CA^{-1}B\] <p>then</p> \[M=\ \begin{bmatrix} I &amp; 0\\ CA^{-1} &amp; I \end{bmatrix} \ \begin{bmatrix} A &amp; 0\\ 0 &amp; S_A \end{bmatrix} \ \begin{bmatrix} I &amp; A^{-1} B\\ 0 &amp; I \end{bmatrix}\] </li> <li> <p>If \(D\) is invertible, then the Schur complement of \(D\) in \(M\) is defined by</p> \[S_D = A - BD^{-1}C\] <p>then</p> \[M=\ \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} \ \begin{bmatrix} S_D &amp; 0\\ 0 &amp; D \end{bmatrix} \ \begin{bmatrix} I &amp; 0\\ D^{-1}C &amp; I \end{bmatrix}\] </li> </ol>]]></content><author><name></name></author><category term="linear-algebra"/><summary type="html"><![CDATA[As a reminder to some bases of the matrix.]]></summary></entry><entry><title type="html">Principal Component Analysis(PCA)</title><link href="https://xiaojxkevin.github.io/blog/2024/ml-pca/" rel="alternate" type="text/html" title="Principal Component Analysis(PCA)"/><published>2024-03-30T00:00:00+00:00</published><updated>2024-03-30T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/ml-pca</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/ml-pca/"><![CDATA[<h2 id="aim">Aim</h2> <p>Given a collection of data points \(y_1, \dots, y_n \in \mathbb{R}^m\), perform a low-dimensional representation</p> \[y_i = Ab_i + c + v_i, \quad i = 1, \dots n,\] <p>where \(A \in \mathbb{R}^{m\times k} \ (k &lt; m)\) to be a basis matrix; \(b_i \in \mathbb{R}^k\) to be the coefficient for \(y_i\); \(c \in \mathbb{R}^m\) is the base or mean in statistics terms; \(v_i\) is the noise or modeling error (assume that it is distributed in Gaussian Distribution).</p> <h2 id="remove-ambiguity">Remove ambiguity</h2> <ol> <li>We can make \(y_i = A(b_i-b_0) + (Ab_0 + c) + v_i\), thus there exists an ambiguity in translation</li> <li>We can make \(y_i = AUU^{-1} b_i + c+v_i\), thus there exists an ambiguity in choosing basis.</li> </ol> <p>As a result, we constriant the free variables by introducing:</p> <ol> <li>\(A^TA=I\) (basis to be orthogonal)</li> <li>\(\sum_{i=1}^n b_i = 0\) (centerlize all coefficients)</li> </ol> <h2 id="from-the-perspective-of-minimizing-errors">From the perspective of minimizing errors</h2> <p>Since \(v_i\) is distributed as Gaussian, it would be equivalent to minimize</p> \[F = \sum_{i=1}^n ||y_i - Ab_i - c||^2_2.\] <p>Compute gradient of \(c\) to be</p> \[\frac{\partial F}{\partial c} = \sum_{i=1}^n2(Ab_i-y_i +c) = 2(c - \frac{1}{n}\sum_{i=1}^n y_i),\] <p>thus we set \(c = \frac{1}{n}\sum_{i=1}^n y_i\), which is to centerlize all data.</p> <p>Define \(\bar{Y} \in \mathbb{R}^{m\times n}\) to be the centerlized data matrix, where each column is a data point. Thus we are minimizing</p> \[F = ||\bar{Y} - AB||_F^2\] <p>Notice that</p> \[\frac{\partial F}{\partial B} = -2A^T\bar{Y} + 2A^TAB = 2(B - A^T\bar{Y})\] <p>set \(B = A^T\bar{Y}\)</p> <p>As a result, we want to minimize</p> \[F = ||\bar{Y} - AA^T\bar{Y}||_F^2 = tr(\bar{Y}^T\bar{Y}) - tr(\bar{Y}^TAA^T\bar{Y})\] <p>At last, it would be equivalent to maximize</p> \[\begin{equation} F = tr(\bar{Y}^TAA^T\bar{Y}) = tr(B^TB) = tr(A^T\bar{Y}\bar{Y}^TA) \end{equation}\] <h2 id="from-the-perspective-of-maximizing-variance">From the perspective of maximizing variance</h2> <p>Notice that the mean of \(b_i\) is zero, thus the variance of \(b_i\) would be</p> \[\sum_{i=1}^n b_i^Tb_i = tr(B^TB)\] <p>As a result, minimizing errors and maximizing variance are equivalent things.</p> <h2 id="find-the-basis-matrix">Find the basis matrix</h2> <p>Define \(A\ =\ \begin{bmatrix} A_{1} &amp; \dotsc &amp; A_{k} \end{bmatrix}\), where \(A_j \in \mathbb{R}^m\) and \(\Sigma = \bar{Y}\bar{Y}^T\) to be the covariance of the centerlized data. we can rewrite the problem as</p> \[\begin{aligned} \max &amp; \ \sum _{j=1}^{n} A_{j}^{T} \Sigma A_{j}\\ s.t. &amp; \ A_{j}^{T} A_{j} =1 \end{aligned}\] <p>Find the Lagrangian dual function and compute gradient \(\partial/\partial A_j\) to be zero:</p> \[2 (\Sigma A_j - \lambda A_j) = 0\] <p>Notice that the equation above is exactly the equation of computing eigenvalues and eigenvectors! As a result, columns of \(A\) are the eigenvetors of the covariance matrix.</p> <h2 id="references">References</h2> <ol> <li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix cookbook for derivatives</a></li> </ol>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[Proof]]></summary></entry><entry><title type="html">Eigen &amp;amp; Matrix</title><link href="https://xiaojxkevin.github.io/blog/2024/eigen/" rel="alternate" type="text/html" title="Eigen &amp;amp; Matrix"/><published>2024-03-29T00:00:00+00:00</published><updated>2024-03-29T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/eigen</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/eigen/"><![CDATA[<p><strong><em>Notice that we will only consider real matrices in the following sections</em></strong></p> <h2 id="basic">Basic</h2> <p>Consider \(A \in M_n\), the basic definition of eigenvalues and eigenvector is \(Av = \lambda v\). The only constriant is that \(v\) <strong>must not be zero vector!</strong> And we construct the <em>characteristic polynomial function</em>:</p> \[\begin{aligned} \det( A-\lambda I) &amp; =( -1)^{n}( \lambda _{1} -\lambda )^{n_{1}} \cdots ( \lambda _{r} -\lambda )^{n_{r}}\\ &amp; =( -1)^{n}\left( \lambda ^{n} -tr( A) \lambda ^{n-1} +\cdots +( -1)^{n}\det( A)\right) \end{aligned}\] <p>notice that the characteristic polynomial function is a polynomial of order \(n\), we can conclude the following theorem:</p> \[\begin{equation} A\ \text{must have } n\text{ eigenvalues (they can be the same)} \end{equation}\] \[\begin{equation} \det(A) = \prod_i^{r} \lambda_i^{n_i} \end{equation}\] \[\begin{equation} tr(A) = \sum_i^{r} n_i\lambda_i \end{equation}\] <p>If \(A\) is diagonalizable, we have</p> \[\begin{equation} A = PDP^{-1} \end{equation}\] <p>where \(D\) is the diagonal matrix and \(P\) is an invertible matrix consists of corresponding eigenvectors. And if \(A\) is symmetric, we can find that \(P\) is orthogonal.</p> <p>For matrix \(A, B \in M_n\), we say that \(A\) and \(B\) are similar if we can find an invertible matrix \(P\) such that \(P^{-1}AP = B\).</p> <p><em>Take a look at References 1-4 for proof and extensions</em></p> <h3 id="a-few-facts-to-know">A few facts to know</h3> <ol> <li> <p><del>If the eigenvalues of a square matrix are all zeros, then the matrix is the zero matrix</del> <br/> Look at this one here: \(\begin{bmatrix} 0 &amp; 1\\ 0 &amp; 0 \end{bmatrix}\)</p> </li> <li> <p>If a <strong>symmetric</strong> matrix has only zero eigenvalues, it is in fact a zero matrix.</p> </li> <li> <p>Inverse dose not imply diagonalization, and vice versa.</p> </li> </ol> <h2 id="eigen--transformations">Eigen &amp; Transformations</h2> <p>One way of understanding matrices is to consider them as some kind of linear transformations, thus we need a set of <em>basis</em> to represent every transformation. And if we choose a distinct set of basis, the representation of the transformation would be distinct as well. As a result, the concept of <strong>similarity</strong> is introduced. The main idea is that if two matrices are similar, we can say that they are the same transformation in distinct representations with respect to corresponding distinct bases. A visual explanation in given below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/24-03-29/similar-480.webp 480w,/assets/img/blogs/24-03-29/similar-800.webp 800w,/assets/img/blogs/24-03-29/similar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/24-03-29/similar.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An explanation for similarity. </div> <p>As a result, from my point of view, what eigen-decomposition is doing is to find a much simpler way to represent a kind of transformation in diagonal(scaling) form under a certain set of basis(the span of eigenvectors).</p> <h2 id="eigenvalues-and-rank">Eigenvalues and Rank</h2> <p>Given a matrix \(A \in \mathbb{R}^{n\times n}\), we have the following conclusions:</p> <ol> <li>If zero is not an eigenvalue of \(A\), then \(A\) is in full rank.</li> <li>\(\dim Eigenspace(\lambda=0, A) + rank(A) = n\) (see Reference 5).</li> </ol> <p><em>For more, please take a look at Reference 6</em></p> <h2 id="references">References</h2> <ol> <li><a href="https://math.stackexchange.com/questions/1471251/why-is-that-an-n-times-n-matrix-have-n-eigenvalues">https://math.stackexchange.com/questions/1471251/why-is-that-an-n-times-n-matrix-have-n-eigenvalues</a></li> <li><a href="https://math.stackexchange.com/questions/507641/show-that-the-determinant-of-a-is-equal-to-the-product-of-its-eigenvalues">https://math.stackexchange.com/questions/507641/show-that-the-determinant-of-a-is-equal-to-the-product-of-its-eigenvalues</a></li> <li><a href="https://math.stackexchange.com/questions/546155/proof-that-the-trace-of-a-matrix-is-the-sum-of-its-eigenvalues">https://math.stackexchange.com/questions/546155/proof-that-the-trace-of-a-matrix-is-the-sum-of-its-eigenvalues</a></li> <li><a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices">https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices</a></li> <li><a href="https://math.stackexchange.com/questions/1349907/what-is-the-relation-between-rank-of-a-matrix-its-eigenvalues-and-eigenvectors">https://math.stackexchange.com/questions/1349907/what-is-the-relation-between-rank-of-a-matrix-its-eigenvalues-and-eigenvectors</a></li> <li><a href="https://www.zhihu.com/question/20882961">https://www.zhihu.com/question/20882961</a></li> </ol>]]></content><author><name></name></author><category term="linear-algebra"/><summary type="html"><![CDATA[Notes on Eigenvalues and Eigenvectors.]]></summary></entry><entry><title type="html">Josephus Problem</title><link href="https://xiaojxkevin.github.io/blog/2024/josephus/" rel="alternate" type="text/html" title="Josephus Problem"/><published>2024-03-27T00:00:00+00:00</published><updated>2024-03-27T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/josephus</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/josephus/"><![CDATA[<h2 id="问题描述">问题描述</h2> <blockquote> <p>社团共有 <code class="language-plaintext highlighter-rouge">n</code> 位成员参与破冰游戏，编号为 <code class="language-plaintext highlighter-rouge">0 ~ n-1</code>。成员们按照编号顺序围绕圆桌而坐。社长抽取一个数字 <code class="language-plaintext highlighter-rouge">m</code>，从 <code class="language-plaintext highlighter-rouge">0</code> 号成员起开始计数，排在第 <code class="language-plaintext highlighter-rouge">m</code> 位的成员离开圆桌，且成员离开后从下一个成员开始计数。请返回游戏结束时最后一位成员（赢家）的编号。</p> </blockquote> <blockquote> <p>保证\(m\geq 1\), \(n \geq 1\) 以及<code class="language-plaintext highlighter-rouge">m, n</code>为<code class="language-plaintext highlighter-rouge">int</code>类型。</p> </blockquote> <h2 id="问题分析">问题分析</h2> <p>首先需要明确一个概念：我们要求的编号与索引是不同的，每次一次迭代的索引都是从0开始。但是注意到在迭代开始的时候，我们的索引与编号是完全一致的。那么就会引申出两种解法。</p> <h3 id="模拟过程">模拟过程</h3> <p>我们不妨建一个循环的链表，即最后链表的<code class="language-plaintext highlighter-rouge">next</code>指向<code class="language-plaintext highlighter-rouge">head</code>，然后不断模拟整个游戏的过程。这个方法必然能稳定得出正确的结果，但其时间复杂度在\(O(n^2)\)，空间复杂度在\(O(n)\)。当数据规模很大时它的表现就会比较差。</p> <h3 id="状态转移">状态转移</h3> <p>对于圆这种可以无限循环的数据结构而言，有一个巧妙的方法是取模，然后构造两个并排的列表（中间用<code class="language-plaintext highlighter-rouge">|</code>隔开）：</p> <p></p> <table> <thead> <tr> <th>0</th> <th>…</th> <th>n-1</th> <th>|</th> <th>0</th> <th>…</th> <th>n-1</th> </tr> </thead> <tbody> <tr> <td>this</td> <td>line</td> <td>is</td> <td>left</td> <td>empty</td> <td>for</td> <td>purpose</td> </tr> </tbody> </table> <p></p> <p>这样一来，给定任意一个要删除的索引<code class="language-plaintext highlighter-rouge">k</code>（<code class="language-plaintext highlighter-rouge">k</code>必然在<code class="language-plaintext highlighter-rouge">0 ~ n-1</code>之间，不失一般性我们假定<code class="language-plaintext highlighter-rouge">k</code>不在边界处） ，我们可以很简单地构造下一阶段的并排列表：</p> <p></p> <table> <thead> <tr> <th>n:</th> <th>0</th> <th>…</th> <th>k</th> <th>…</th> <th>|</th> <th>0</th> <th>…</th> <th>k</th> <th>…</th> </tr> </thead> <tbody> <tr> <td><strong>n-1:</strong></td> <td><strong>k+1</strong></td> <td><strong>…</strong></td> <td><strong>0</strong></td> <td><strong>…</strong></td> <td><strong>k-1</strong></td> <td><strong>|</strong></td> <td><strong>k+1</strong></td> <td><strong>…</strong></td> <td> </td> </tr> </tbody> </table> <p></p> <p>我们可以总结出一个规律：对于每一次迭代，我们就相当于扔掉前面<code class="language-plaintext highlighter-rouge">k</code>个元素，再向后数对应<code class="language-plaintext highlighter-rouge">n-1</code>个，数完了以后再复制这<code class="language-plaintext highlighter-rouge">n-1</code>个构造出并排的列表。</p> <p>定义\(f(x):\) 成员个数为\(x\)时赢家在并排列表中的 <strong>索引</strong>（由于是并排的，我们只需要关注前半部分就OK了），注意到赢家是不会被删除的，因此\(x\)的取值在 \(1, 2, 3, ..., n\)。</p> <p>注意到\(f(x)\)有两个特殊之处：</p> <ol> <li>\(f(1) = 0\);</li> <li>\(f(n)\)就是我们最终要求的赢家的编号。</li> </ol> <p>那么现在的任务就是去求得 \(f(n)\)，自然地，我们希望能找到一个合适的递推公式，这样就可以求解了。</p> <p>当前成员人数为<code class="language-plaintext highlighter-rouge">t</code>时，我们用<code class="language-plaintext highlighter-rouge">X</code>表示被删除的编号，<code class="language-plaintext highlighter-rouge">S</code>表示即将成为索引为0的编号，<code class="language-plaintext highlighter-rouge">*</code>表示赢家的编号。具体过程如下：</p> <p></p> <table> <thead> <tr> <th><code class="language-plaintext highlighter-rouge">t:</code></th> <th>…</th> <th>X</th> <th>S</th> <th>…</th> <th>*</th> <th>…</th> <th>|</th> <th>…</th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">t-1:</code></strong></td> <td><strong>S</strong></td> <td><strong>…</strong></td> <td><strong>*</strong></td> <td><strong>…</strong></td> <td><strong>|</strong></td> <td><strong>…</strong></td> <td> </td> <td> </td> </tr> </tbody> </table> <p></p> <p>注意到<code class="language-plaintext highlighter-rouge">S</code>在<code class="language-plaintext highlighter-rouge">t-1</code>的索引为<code class="language-plaintext highlighter-rouge">0</code>，在<code class="language-plaintext highlighter-rouge">t</code>的索引为 \(m\%t\)。于是乎，我们能找到两个状态之间的平移量：\(m\%t\)。已知<code class="language-plaintext highlighter-rouge">*</code>的索引在<code class="language-plaintext highlighter-rouge">t-1</code>状态时为\(f(t-1)\)，那么我们可以恢复出它在<code class="language-plaintext highlighter-rouge">t</code>状态时的索引：\(m\%t+f(t-1)\)。为防止越界，我们取模，得到状态转移方程</p> \[f(t) = [m\%t+f(t-1)]\%t\] <p>鉴于我们已知\(f(1)\)，我们可以直接写一个循环求解:</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">solution</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">f</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span> <span class="o">%</span> <span class="n">i</span> <span class="o">+</span> <span class="n">f</span><span class="p">)</span> <span class="o">%</span> <span class="n">i</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">f</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>该算法时间复杂度\(O(n)\)，空间复杂度为\(O(1)\)。</p> <h2 id="参考资料">参考资料</h2> <ol> <li><a href="https://leetcode.cn/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solutions/177639/javajie-jue-yue-se-fu-huan-wen-ti-gao-su-ni-wei-sh/">https://leetcode.cn/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solutions/177639/javajie-jue-yue-se-fu-huan-wen-ti-gao-su-ni-wei-sh/</a></li> <li><a href="https://oi-wiki.org/misc/josephus/">https://oi-wiki.org/misc/josephus/</a></li> </ol>]]></content><author><name></name></author><category term="algorithm"/><summary type="html"><![CDATA[The solution.]]></summary></entry><entry><title type="html">Linear Programming</title><link href="https://xiaojxkevin.github.io/blog/2024/opt-lp/" rel="alternate" type="text/html" title="Linear Programming"/><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/opt-lp</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/opt-lp/"><![CDATA[<p>Redirecting to <code class="language-plaintext highlighter-rouge">Linear_Programming.pdf</code></p>]]></content><author><name></name></author><category term="optimization"/><summary type="html"><![CDATA[Notes for course SI152 at ShanghaiTech]]></summary></entry><entry><title type="html">Pose Graph 1</title><link href="https://xiaojxkevin.github.io/blog/2024/pose-graph-1/" rel="alternate" type="text/html" title="Pose Graph 1"/><published>2024-03-22T00:00:00+00:00</published><updated>2024-03-22T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/pose-graph-1</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/pose-graph-1/"><![CDATA[<h3 id="notations">Notations</h3> <p><em>please ignore this if you haven’t seen contents below</em></p> <ul> <li> <p>\(x_{i} =[ t_{x} ,t_{y} ,t_{z} ,\theta_{x} ,\theta_{y} ,\theta_{z}]^{T} \in \mathbb{R}^6\) to be a state, and \(T_{i}\) is the corresponding transformation function in \(SE(3)\), where</p> \[T_{i} \ =\ \begin{bmatrix} R_{i} &amp; t_{i}\\ 0 &amp; 1 \end{bmatrix}\] </li> <li> <p><em>t2v()</em> is a function that maps a state to its corresponding transformation matrix; and <em>v2t()</em> is exactly the inverse function.</p> </li> <li> <p>\(x = [x_{1}^{T} , ..., x_{n}^{T}]\) to be the state vector of \(n\) states.</p> </li> <li> <p>\(\displaystyle Z_{ij} \ \in \ SE( 3)\) to be the measured transformation matrix from scan \(\displaystyle i\) to scan \(\displaystyle j\) (like the reslult of using ICP to match two scans). And we define \(\displaystyle z_{ij} \ =\ t2v( Z_{ij})\).</p> \[Z_{ij} \ =\ \begin{bmatrix} R_{ij} &amp; t_{ij}\\ 0 &amp; 1 \end{bmatrix}\] </li> <li> <p>Define \(\displaystyle \tilde{Z}_{ij} \ =\ T_{i}^{-1} T_{j}\) to be the relative pose via two states, and \(\displaystyle \tilde{z}_{ij} \ =\ t2v\left(\tilde{Z}_{ij}\right)\).</p> </li> <li> <p>Define \(\displaystyle e_{ij}( x) \ =\ \tilde{z}_{ij} -z_{ij}\) to be gap between the relative pose given by two states and the measured relative pose. (A number of blogs online do not define this way, but I think this would be a more efficient way for it has a simplier form of Jacobian).</p> </li> </ul> <h3 id="general-idea">General idea</h3> <p>Since we use a <em>graph</em> to optimize things, it is clear that we have to define two most important things in any kind of graphs: vertex and edge.</p> <ul> <li> <p>A vertex is a state, a random variable in probability, which needs to be optimized. In pose graph, it would be a state of pose \(\displaystyle x_{i}\).</p> </li> <li> <p>An edge is used to constraint two verteces. In pose graph, it could be a relative pose given by ICP matching. Notice that we do not optimize edges.</p> </li> </ul> <p>A nice metaphor is that consider all verteces are objectes connected by spring (edges), and at the beginning the system is loose and stable. Then we introduce some more spring to the system, and this makes almost all spring to be active (store a large amount of energy). The goal of optimization is to minimize this energy to make the system stable again.</p> <h3 id="define-loss">Define Loss</h3> <p>Our goal is to minimize all \(\displaystyle e_{ij}( x)\). Assume that \(\displaystyle e_{ij}( x) \ \sim \ \mathcal{N}( 0,\ \Omega _{ij})\), and all errors are i.i.d., then it is equivalent to find the MLE:</p> \[G( e_{ij}( x)) \ =\ \prod _{i,j}\frac{1}{( 2\pi )^{3} |\Omega _{ij} |^{1/2}} \ \exp\left( -\frac{1}{2} e_{ij}^{T}( x) \Omega _{ij} e_{ij}( x)\right)\] <p>Notice that</p> \[\ln( G( e_{ij}( x))) \ =\ \sum \frac{1}{( 2\pi )^{3} |\Omega _{ij} |^{1/2}} \ -\ \frac{1}{2}\sum _{i,j} e_{ij}^{T}( x) \Omega _{ij} e_{ij}( x)\] <p>it would be the same to minimize</p> \[F( x) \ =\ \sum _{i,j} e_{ij}^{T}( x) \Omega _{ij} e_{ij}( x)\] <p>Of course, we can assume a distinct distribution, say Laplace distribution, and then the loss function would be different as above. In addtion, in order to make system robust to outliers, we can also apply Huber loss, which is widely used in Deep Learning:</p> \[Huber( e) \ =\ \begin{cases} \frac{1}{2} e^{2} , &amp; |e|\leq \delta \\ \delta \left( |e|\ -\ \frac{1}{2} \delta \right) , &amp; \text{otherwise} \end{cases}\] <p>To make things simple, we will assume Gaussian distribution.</p> <h5 id="q-what-is-displaystyle-omega-_ij-and-how-to-initialize-it">Q: what is \(\displaystyle \Omega _{ij}\) and how to initialize it?</h5> <p>A widely accepted answer is that \(\displaystyle \Omega _{ij}\) accounts for the uncertainty of the measurements, which is the inverse of the covariance matrix. Recall that measurements are obtained by ICP matching, thus we have a matched point set \(\displaystyle \{p_{i} ,q_{i}\}\) of two scans. And ICP algorithms give us a minimum value of the matching error</p> \[E(e_{ij}( x)) \ =\ \sum ||p_{i} \ -t2v( z_{ij}) *q_{i} ||_{2}^{2}\] <p>then we can find \(\displaystyle \Omega _{ij} \ =\ JJ^{T} \in \ M_{6}\), where</p> \[\displaystyle J\ =\ \frac{\partial E( e_{ij}( x))}{\partial e_{ij}( x)} \in \ \mathbb{R}^{6}\] <p>to be the Jacobian matrix.</p> <p>Of course, we can set it to identity or experimental values, there are more than one initialization.</p> <h3 id="main-process">Main Process</h3> <p>For the rest of the part, I advise you to look at the paper <a href="https://ieeexplore.ieee.org/document/5681215">A Tutorial on Graph-Based SLAM</a>.</p> <p>The reason why I do not write one myself is that currently all my ideas are inherited from that paper, i.e. I have not created some new ideas. Then it would be a nice choice to just provide the original paper to readers.</p> <p>But there’s one thing that I want to mention, it is that the gradient should instead be</p> \[\begin{array}{l} A_{ij} = \frac{\partial e_{ij}( x)}{\partial x_{i}^{T}} = \begin{bmatrix} -R_{i}^{T} &amp; \frac{\partial R_{i}^T}{\partial \theta _{i}}( t_{j} -t_{i})\\ 0 &amp; -1 \end{bmatrix}\\ B_{ij} = \frac{\partial e_{ij}( x)}{\partial x_{j}^{T}} = \begin{bmatrix} R_{i}^{T} &amp; 0\\ 0 &amp; 1 \end{bmatrix} \end{array}\] <p>since we have dropped \(z_{ij}\).</p> <h3 id="related-blogs">Related blogs</h3> <ul> <li><a href="https://github.com/xiaojxkevin/learning_slam/tree/main/pose-graph/project">Project of mine</a></li> <li><a href="https://blog.csdn.net/u010507357/article/details/108540110">blog-csdn</a></li> <li><a href="https://robotics.stackexchange.com/questions/22451/calculate-information-matrix-for-graph-slam">information-matrix</a></li> </ul>]]></content><author><name></name></author><category term="slam"/><summary type="html"><![CDATA[Notes of pose graph optimization]]></summary></entry><entry><title type="html">Clearing Disk Space on Ubuntu 22.04</title><link href="https://xiaojxkevin.github.io/blog/2024/clear-up-disk/" rel="alternate" type="text/html" title="Clearing Disk Space on Ubuntu 22.04"/><published>2024-03-14T00:00:00+00:00</published><updated>2024-03-14T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/clear-up-disk</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/clear-up-disk/"><![CDATA[<h3 id="background">Background</h3> <p>The idea is simple, it is that I do not have enough disk space, since I only allocated around 90GB for my Ubuntu system. And in fact, there are a lot of abundant files that can be removed.</p> <p><strong>A Useful Command:</strong> <code class="language-plaintext highlighter-rouge">du -hx --max-depth=1 --threshold=800M</code>, which helps us find directories that takes up over 800MB, and these would be our targets.</p> <h3 id="root-">Root <code class="language-plaintext highlighter-rouge">/</code></h3> <ol> <li> <p>I first find out that there are a number of kernel instances in <code class="language-plaintext highlighter-rouge">/usr/lib/modules/</code>, and the fact is that I only need one or two of them. Then I follow the instructions in <a href="#reference">ref1</a> and obtain a few more GBs.</p> </li> <li> <p>Remove Snap (in fact I can find alternatives). Details are in <a href="#reference">ref2</a>.</p> </li> <li> <p>Big journels in <code class="language-plaintext highlighter-rouge">/var/log/journal</code>!. Details are in <a href="#reference">ref3</a></p> </li> </ol> <h3 id="home-">Home <code class="language-plaintext highlighter-rouge">~</code></h3> <ol> <li>I have been using <code class="language-plaintext highlighter-rouge">VSCode</code> since first year at college. Although it is great, there are quite a lot issues related to storage. <ul> <li> <p>Do you use the <code class="language-plaintext highlighter-rouge">cpp-extension</code> in VSCode? If yes, you may need to consider cpptools in <code class="language-plaintext highlighter-rouge">~/.cache/vscode-cpptools</code>. If you want to shorten the space usage of it, please go to settings and edit cache size.</p> </li> <li> <p>Please check <code class="language-plaintext highlighter-rouge">~/.config/Code/User/workspaceStorage</code>, it may also surprise you. See <a href="#reference">ref4</a> for more details.</p> </li> </ul> </li> </ol> <h3 id="reference">Reference</h3> <ul> <li><a href="https://serverfault.com/questions/1098556/how-to-cleanup-usr-lib-modules-and-usr-lib-x86-64-linux-gnu">ref1</a></li> <li><a href="https://sysin.org/blog/ubuntu-remove-snap/">ref2</a></li> <li><a href="https://askubuntu.com/questions/1238214/big-var-log-journal">ref3</a></li> <li><a href="https://www.jianshu.com/p/7497160db18b">ref4</a></li> </ul>]]></content><author><name></name></author><category term="system"/><summary type="html"><![CDATA[A Record of how I clear up disk space]]></summary></entry></feed>