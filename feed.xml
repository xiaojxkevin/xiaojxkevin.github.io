<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://xiaojxkevin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xiaojxkevin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-22T10:00:19+00:00</updated><id>https://xiaojxkevin.github.io/feed.xml</id><title type="html">Jinxi’s Home</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Pose Graph 1</title><link href="https://xiaojxkevin.github.io/blog/2024/slam/" rel="alternate" type="text/html" title="Pose Graph 1"/><published>2024-03-22T00:00:00+00:00</published><updated>2024-03-22T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/slam</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/slam/"><![CDATA[<h3 id="notations">Notations</h3> <p><em>please ignore this if you haven’t seen contents below</em></p> <ul> <li> <p>\(x_{i} =[ t_{x} ,t_{y} ,t_{z} ,\theta_{x} ,\theta_{y} ,\theta_{z}]^{T} \in \mathbb{R}^6\) to be a state, and \(T_{i}\) is the corresponding transformation function in \(SE(3)\), where</p> \[T_{i} \ =\ \begin{bmatrix} R_{i} &amp; t_{i}\\ 0 &amp; 1 \end{bmatrix}\] </li> <li> <p><em>t2v()</em> is a function that maps a state to its corresponding transformation matrix; and <em>v2t()</em> is exactly the inverse function.</p> </li> <li> <p>\(x = [x_{1}^{T} , ..., x_{n}^{T}]\) to be the state vector of \(n\) states.</p> </li> <li> <p>\(\displaystyle Z_{ij} \ \in \ SE( 3)\) to be the measured transformation matrix from scan \(\displaystyle i\) to scan \(\displaystyle j\) (like the reslult of using ICP to match two scans). And we define \(\displaystyle z_{ij} \ =\ t2v( Z_{ij})\).</p> \[Z_{ij} \ =\ \begin{bmatrix} R_{ij} &amp; t_{ij}\\ 0 &amp; 1 \end{bmatrix}\] </li> <li> <p>Define \(\displaystyle \tilde{Z}_{ij} \ =\ T_{i}^{-1} T_{j}\) to be the relative pose via two states, and \(\displaystyle \tilde{z}_{ij} \ =\ t2v\left(\tilde{Z}_{ij}\right)\).</p> </li> <li> <p>Define \(\displaystyle e_{ij}( x) \ =\ \tilde{z}_{ij} -z_{ij}\) to be gap between the relative pose given by two states and the measured relative pose. (A number of blogs online do not define this way, but I think this would be a more efficient way for it has a simplier form of Jacobian).</p> </li> </ul> <h3 id="general-idea">General idea</h3> <p>Since we use a <em>graph</em> to optimize things, it is clear that we have to define two most important things in any kind of graphs: vertex and edge.</p> <ul> <li> <p>A vertex is a state, a random variable in probability, which needs to be optimized. In pose graph, it would be a state of pose \(\displaystyle x_{i}\).</p> </li> <li> <p>An edge is used to constraint two verteces. In pose graph, it could be a relative pose given by ICP matching. Notice that we do not optimize edges.</p> </li> </ul> <p>A nice metaphor is that consider all verteces are objectes connected by spring (edges), and at the beginning the system is loose and stable. Then we introduce some more spring to the system, and this makes almost all spring to be active (store a large amount of energy). The goal of optimization is to minimize this energy to make the system stable again.</p> <h3 id="define-loss">Define Loss</h3> <p>Our goal is to minimize all \(\displaystyle e_{ij}( x)\). Assume that \(\displaystyle e_{ij}( x) \ \sim \ \mathcal{N}( 0,\ \Omega _{ij})\), and all states are i.i.d., then it is equivalent to find the MLE:</p> \[G( e_{ij}( x)) \ =\ \prod _{i,j}\frac{1}{( 2\pi )^{3} |\Omega _{ij} |^{1/2}} \ \exp\left( -\frac{1}{2} e_{ij}^{T}( x) \Omega _{ij} e_{ij}( x)\right)\] <p>Notice that</p> \[\ln( G( e_{ij}( x))) \ =\ \sum \frac{1}{( 2\pi )^{3} |\Omega _{ij} |^{1/2}} \ -\ \frac{1}{2}\sum _{i,j} e_{ij}^{T}( x) \Omega _{ij} e_{ij}( x)\] <p>it would be the same to minimize</p> \[F( x) \ =\ \sum _{i,j} e_{ij}^{T}( x) \Omega _{ij} e_{ij}( x)\] <p>Of course, we can assume a distinct distribution, say Laplace distribution, and then the loss function would be different as above. In addtion, in order to make system robust to outliers, we can also apply Huber loss, which is widely used in Deep Learning:</p> \[Huber( e) \ =\ \begin{cases} \frac{1}{2} e^{2} , &amp; |e|\leq \delta \\ \delta \left( |e|\ -\ \frac{1}{2} \delta \right) , &amp; \text{otherwise} \end{cases}\] <p>To make things simple, we will assume Gaussian distribution.</p> <h5 id="q-what-is-displaystyle-omega-_ij-and-how-to-initialize-it">Q: what is \(\displaystyle \Omega _{ij}\) and how to initialize it?</h5> <p>A widely accepted answer is that \(\displaystyle \Omega _{ij}\) accounts for the uncertainty of the measurements, which is the inverse of the covariance matrix. Recall that measurements are obtained by ICP matching, thus we have a matched point set \(\displaystyle \{p_{i} ,q_{i}\}\) of two scans. And ICP algorithms give us a minimum value of the matching error</p> \[E(e_{ij}( x)) \ =\ \sum ||p_{i} \ -t2v( z_{ij}) *q_{i} ||_{2}^{2}\] <p>then we can find \(\displaystyle \Omega _{ij} \ =\ JJ^{T} \in \ M_{6}\), where</p> \[\displaystyle J\ =\ \frac{\partial E( e_{ij}( x))}{\partial e_{ij}( x)} \in \ \mathbb{R}^{6}\] <p>to be the Jacobian matrix.</p> <p>Of course, we can set it to identity or experimental values, there are more than one initialization.</p> <h3 id="main-process">Main Process</h3> <p>For the rest of the part, I advise you to look at the paper <a href="https://ieeexplore.ieee.org/document/5681215">A Tutorial on Graph-Based SLAM</a>.</p> <p>The reason why I do not write one myself is that currently all my ideas are inherited from that paper, i.e. I have not created some new ideas. Then it would be a nice choice to just provide the original paper to readers.</p> <p>But there’s one thing that I want to mention, it is that the gradient should instead be</p> \[\begin{array}{l} A_{ij} = \frac{\partial e_{ij}( x)}{\partial x_{i}^{T}} = \begin{bmatrix} -R_{i}^{T} &amp; \frac{\partial R_{i}^T}{\partial \theta _{i}}( t_{j} -t_{i})\\ 0 &amp; -1 \end{bmatrix}\\ B_{ij} = \frac{\partial e_{ij}( x)}{\partial x_{j}^{T}} = \begin{bmatrix} R_{i}^{T} &amp; 0\\ 0 &amp; 1 \end{bmatrix} \end{array}\] <p>since we have dropped \(z_{ij}\).</p> <h3 id="related-blogs">Related blogs</h3> <ul> <li><a href="https://github.com/xiaojxkevin/learning_slam/tree/main/pose-graph/project">Project of mine</a></li> <li><a href="https://blog.csdn.net/u010507357/article/details/108540110">blog-csdn</a></li> <li><a href="https://robotics.stackexchange.com/questions/22451/calculate-information-matrix-for-graph-slam">information-matrix</a></li> </ul>]]></content><author><name></name></author><category term="slam"/><summary type="html"><![CDATA[Notes of pose graph optimization]]></summary></entry><entry><title type="html">Clearing Disk Space on Ubuntu 22.04</title><link href="https://xiaojxkevin.github.io/blog/2024/system/" rel="alternate" type="text/html" title="Clearing Disk Space on Ubuntu 22.04"/><published>2024-03-14T00:00:00+00:00</published><updated>2024-03-14T00:00:00+00:00</updated><id>https://xiaojxkevin.github.io/blog/2024/system</id><content type="html" xml:base="https://xiaojxkevin.github.io/blog/2024/system/"><![CDATA[<h3 id="background">Background</h3> <p>The idea is simple, it is that I do not have enough disk space, since I only allocated around 90GB for my Ubuntu system. And in fact, there are a lot of abundant files that can be removed.</p> <p><strong>A Useful Command:</strong> <code class="language-plaintext highlighter-rouge">du -hx --max-depth=1 --threshold=800M</code>, which helps us find directories that takes up over 800MB, and these would be our targets.</p> <h3 id="root-">Root <code class="language-plaintext highlighter-rouge">/</code></h3> <ol> <li> <p>I first find out that there are a number of kernel instances in <code class="language-plaintext highlighter-rouge">/usr/lib/modules/</code>, and the fact is that I only need one or two of them. Then I follow the instructions in <a href="#reference">ref1</a> and obtain a few more GBs.</p> </li> <li> <p>Remove Snap (in fact I can find alternatives). Details are in <a href="#reference">ref2</a>.</p> </li> <li> <p>Big journels in <code class="language-plaintext highlighter-rouge">/var/log/journal</code>!. Details are in <a href="#reference">ref3</a></p> </li> </ol> <h3 id="home-">Home <code class="language-plaintext highlighter-rouge">~</code></h3> <ol> <li>I have been using <code class="language-plaintext highlighter-rouge">VSCode</code> since first year at college. Although it is great, there are quite a lot issues related to storage. <ul> <li> <p>Do you use the <code class="language-plaintext highlighter-rouge">cpp-extension</code> in VSCode? If yes, you may need to consider cpptools in <code class="language-plaintext highlighter-rouge">~/.cache/vscode-cpptools</code>. If you want to shorten the space usage of it, please go to settings and edit cache size.</p> </li> <li> <p>Please check <code class="language-plaintext highlighter-rouge">~/.config/Code/User/workspaceStorage</code>, it may also surprise you. See <a href="#reference">ref4</a> for more details.</p> </li> </ul> </li> </ol> <h3 id="reference">Reference</h3> <ul> <li><a href="https://serverfault.com/questions/1098556/how-to-cleanup-usr-lib-modules-and-usr-lib-x86-64-linux-gnu">ref1</a></li> <li><a href="https://sysin.org/blog/ubuntu-remove-snap/">ref2</a></li> <li><a href="https://askubuntu.com/questions/1238214/big-var-log-journal">ref3</a></li> <li><a href="https://www.jianshu.com/p/7497160db18b">ref4</a></li> </ul>]]></content><author><name></name></author><category term="system"/><summary type="html"><![CDATA[A Record of how I clear up disk space]]></summary></entry></feed>